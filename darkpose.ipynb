{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8430058,"sourceType":"datasetVersion","datasetId":5020200}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yacs ultralytics","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:05:54.006667Z","iopub.execute_input":"2024-05-18T13:05:54.007074Z","iopub.status.idle":"2024-05-18T13:06:06.314231Z","shell.execute_reply.started":"2024-05-18T13:05:54.007042Z","shell.execute_reply":"2024-05-18T13:06:06.313111Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Requirement already satisfied: yacs in /opt/conda/lib/python3.10/site-packages (0.1.8)\nRequirement already satisfied: ultralytics in /opt/conda/lib/python3.10/site-packages (8.2.17)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from yacs) (6.0.1)\nRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (10.3.0)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: thop>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.1.1.post2209072238)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\nimport cv2\nimport numpy as np\nimport os\nimport logging\nfrom yacs.config import CfgNode as CN\nimport math\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision\nimport torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:06:06.316276Z","iopub.execute_input":"2024-05-18T13:06:06.316608Z","iopub.status.idle":"2024-05-18T13:06:06.323478Z","shell.execute_reply.started":"2024-05-18T13:06:06.316578Z","shell.execute_reply":"2024-05-18T13:06:06.322583Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"cfg = CN()\n\ncfg.OUTPUT_DIR = 'output'\ncfg.LOG_DIR = 'log'\ncfg.DATA_DIR = ''\ncfg.GPUS = (0,)  # Updated\ncfg.WORKERS = 24\ncfg.PRINT_FREQ = 100\ncfg.AUTO_RESUME = True  # Updated\ncfg.PIN_MEMORY = True\ncfg.RANK = 0\n\n# Cudnn related params\ncfg.CUDNN = CN()\ncfg.CUDNN.BENCHMARK = True  # Updated\ncfg.CUDNN.DETERMINISTIC = False  # Updated\ncfg.CUDNN.ENABLED = True  # Updated\n\n# common params for NETWORK\ncfg.MODEL = CN()\ncfg.MODEL.NAME = 'pose_hrnet'\ncfg.MODEL.INIT_WEIGHTS = True\ncfg.MODEL.PRETRAINED = '/kaggle/input/darkpose/w32_256Ã—256.pth'  # Updated\ncfg.MODEL.NUM_JOINTS = 16\ncfg.MODEL.TAG_PER_JOINT = True\ncfg.MODEL.TARGET_TYPE = 'gaussian'\ncfg.MODEL.IMAGE_SIZE = [256, 256]\ncfg.MODEL.HEATMAP_SIZE = [64, 64]\ncfg.MODEL.SIGMA = 2\n\ncfg.MODEL.EXTRA = CN()\ncfg.MODEL.EXTRA.NUM_FEATURES = 256\ncfg.MODEL.EXTRA.NUM_STACKS = 8\ncfg.MODEL.EXTRA.NUM_BLOCKS = 1\ncfg.MODEL.EXTRA.NUM_CLASSES = 16\ncfg.MODEL.EXTRA.PRETRAINED_LAYERS = [\n  'conv1',\n  'bn1',\n  'conv2',\n  'bn2',\n  'layer1',\n  'transition1',\n  'stage2',\n  'transition2',\n  'stage3',\n  'transition3',\n  'stage4'\n]\ncfg.MODEL.EXTRA.FINAL_CONV_KERNEL = 1\n\ncfg.MODEL.EXTRA.STAGE2 = CN()\ncfg.MODEL.EXTRA.STAGE2.NUM_MODULES = 1\ncfg.MODEL.EXTRA.STAGE2.NUM_BRANCHES = 2\ncfg.MODEL.EXTRA.STAGE2.NUM_BLOCKS = [4, 4]\ncfg.MODEL.EXTRA.STAGE2.NUM_CHANNELS = [32, 64]\ncfg.MODEL.EXTRA.STAGE2.BLOCK = 'BASIC'\ncfg.MODEL.EXTRA.STAGE2.FUSE_METHOD = 'SUM'\n\ncfg.MODEL.EXTRA.STAGE3 = CN()\ncfg.MODEL.EXTRA.STAGE3.NUM_MODULES = 4\ncfg.MODEL.EXTRA.STAGE3.NUM_BRANCHES = 3\ncfg.MODEL.EXTRA.STAGE3.NUM_BLOCKS = [4, 4, 4]\ncfg.MODEL.EXTRA.STAGE3.NUM_CHANNELS = [32, 64, 128]\ncfg.MODEL.EXTRA.STAGE3.BLOCK = 'BASIC'\ncfg.MODEL.EXTRA.STAGE3.FUSE_METHOD = 'SUM'\n\ncfg.MODEL.EXTRA.STAGE4 = CN()\ncfg.MODEL.EXTRA.STAGE4.NUM_MODULES = 3\ncfg.MODEL.EXTRA.STAGE4.NUM_BRANCHES = 4\ncfg.MODEL.EXTRA.STAGE4.NUM_BLOCKS = [4, 4, 4, 4]\ncfg.MODEL.EXTRA.STAGE4.NUM_CHANNELS = [32, 64, 128, 256]\ncfg.MODEL.EXTRA.STAGE4.BLOCK = 'BASIC'\ncfg.MODEL.EXTRA.STAGE4.FUSE_METHOD = 'SUM'\n\ncfg.LOSS = CN()\ncfg.LOSS.USE_TARGET_WEIGHT = True\ncfg.LOSS.USE_DIFFERENT_JOINTS_WEIGHT = False\n# DATASET related params\ncfg.DATASET = CN()\ncfg.DATASET.ROOT = '/kaggle/input/mpii-2014'  # Updated\ncfg.DATASET.DATASET = 'mpii'\ncfg.DATASET.TRAIN_SET = 'train'\ncfg.DATASET.TEST_SET = 'valid'\ncfg.DATASET.DATA_FORMAT = 'jpg'\ncfg.DATASET.HYBRID_JOINTS_TYPE = ''\ncfg.DATASET.SELECT_DATA = False\n\n# training data augmentation\ncfg.DATASET.COLOR_RGB = True  # Updated\ncfg.DATASET.FLIP = True  # Updated\ncfg.DATASET.NUM_JOINTS_HALF_BODY = 8  # Updated\ncfg.DATASET.PROB_HALF_BODY = -1.0  # Updated\ncfg.DATASET.ROT_FACTOR = 30  # Updated\ncfg.DATASET.SCALE_FACTOR = 0.25  # Updated\n\n# train\ncfg.TRAIN = CN()\n\ncfg.TRAIN.LR_FACTOR = 0.1\ncfg.TRAIN.LR_STEP = [170, 200]\ncfg.TRAIN.LR = 0.001\n\ncfg.TRAIN.OPTIMIZER = 'adam'\ncfg.TRAIN.MOMENTUM = 0.9\ncfg.TRAIN.WD = 0.0001\ncfg.TRAIN.NESTEROV = False\ncfg.TRAIN.GAMMA1 = 0.99\ncfg.TRAIN.GAMMA2 = 0.0\n\ncfg.TRAIN.BEGIN_EPOCH = 0\ncfg.TRAIN.END_EPOCH = 10  # Updated\n\ncfg.TRAIN.RESUME = False\ncfg.TRAIN.CHECKPOINT = ''\n\ncfg.TRAIN.BATCH_SIZE_PER_GPU = 32\ncfg.TRAIN.SHUFFLE = True\n\n# testing\ncfg.TEST = CN()\ncfg.TEST.MODEL_FILE = '/kaggle/input/darkpose/w32_256Ã—256.pth'  # Updated\ncfg.TEST.BATCH_SIZE_PER_GPU = 32\ncfg.TEST.FLIP_TEST = True\ncfg.TEST.POST_PROCESS = True\ncfg.TEST.BLUR_KERNEL = 11  # Updated\ncfg.TEST.SHIFT_HEATMAP = True\ncfg.TEST.USE_GT_BBOX = False\n\n# debug\ncfg.DEBUG = CN()\ncfg.DEBUG.DEBUG = False  # Updated\ncfg.DEBUG.SAVE_BATCH_IMAGES_GT = False  # Updated\ncfg.DEBUG.SAVE_BATCH_IMAGES_PRED = False  # Updated\ncfg.DEBUG.SAVE_HEATMAPS_GT = False  # Updated\ncfg.DEBUG.SAVE_HEATMAPS_PRED = False\n\n# model name\nmodel_name = \"w32_256x256_adam_lr1e-3\"\n\nCTX = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEBUG = True\ncudnn.benchmark = cfg.CUDNN.BENCHMARK\ntorch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\ntorch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n\nMPII_KEYPOINT_INDEXES = {\n  0: \"right ankle\",\n  1: \"right knee\",\n  2: \"right hip\", \n  3: \"left hip\", \n  4: \"left knee\", \n  5: \"left ankle\",\n  6: \"pelvis\", \n  7: \"thorax\", \n  8: \"upper neck\", \n  9: \"head top\", \n  10: \"right wrist\",\n  11: \"right elbow\", \n  12: \"right shoulder\", \n  13: \"left shoulder\", \n  14: \"left elbow\",\n  15: \"left wrist\"\n}\nNUM_KPTS = len(MPII_KEYPOINT_INDEXES)\nSKELETON = {\n  \"left_lower_leg\": [0, 1], \n  \"left_thigh\": [2, 1], \n  \"left_hip\": [2, 6], \n  \"right_lower_leg\": [5, 4],\n  \"right_thigh\": [3, 4],\n  \"right_hip\": [3, 6],\n  \"torso\": [6, 7], \n  \"neck\": [7, 8],             #actually it's thorax - upper neck \n  \"head\": [8, 9],\n  \"right_forearm\": [10, 11],\n  \"right_upper_arm\": [11, 12], \n  \"right_shoulder\": [12, 7],\n  \"left_forearm\": [15, 14],\n  \"left_upper_arm\": [14, 13], \n  \"left_shoulder\": [13, 7]\n}\nSQUAT_PART = [\"left_lower_leg\", \"left_thigh\", \"right_lower_leg\",\n              \"right_thigh\", \"torso\"]\nSQUAT_KEYPART = [\n  [\"left_lower_leg\", \"left_thigh\"],\n  [\"right_lower_leg\", \"right_thigh\"]\n]\nSTAGE_ANGLE = [[180, 170], [170, 155],\n               [155, 137], [137, 114],\n               [114, 0], [0, 114],\n               [114, 148], [148, 168],\n               [168, 180]]\nCOLOR = {\n  \"red\": [0, 0, 255],\n  \"blue\": [255, 0, 0],\n  \"green\": [0, 255, 0]\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:06:06.324937Z","iopub.execute_input":"2024-05-18T13:06:06.325236Z","iopub.status.idle":"2024-05-18T13:06:06.359139Z","shell.execute_reply.started":"2024-05-18T13:06:06.325211Z","shell.execute_reply":"2024-05-18T13:06:06.358284Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Function definition","metadata":{}},{"cell_type":"code","source":"BN_MOMENTUM = 0.1\nlogger = logging.getLogger(__name__)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(True)\n\n    def _check_branches(self, num_branches, blocks, num_blocks,\n                        num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_BLOCKS({})'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_CHANNELS({})'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = 'NUM_BRANCHES({}) <> NUM_INCHANNELS({})'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n           self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(\n                    num_channels[branch_index] * block.expansion,\n                    momentum=BN_MOMENTUM\n                ),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.num_inchannels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample\n            )\n        )\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.num_inchannels[branch_index],\n                    num_channels[branch_index]\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels)\n            )\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_inchannels[j],\n                                num_inchannels[i],\n                                1, 1, 0, bias=False\n                            ),\n                            nn.BatchNorm2d(num_inchannels[i]),\n                            nn.Upsample(scale_factor=2**(j-i), mode='nearest')\n                        )\n                    )\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i-j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3)\n                                )\n                            )\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(\n                                nn.Sequential(\n                                    nn.Conv2d(\n                                        num_inchannels[j],\n                                        num_outchannels_conv3x3,\n                                        3, 2, 1, bias=False\n                                    ),\n                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n                                    nn.ReLU(True)\n                                )\n                            )\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    'BASIC': BasicBlock,\n    'BOTTLENECK': Bottleneck\n}\n\n\nclass PoseHighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, **kwargs):\n        self.inplanes = 64\n        extra = cfg.MODEL.EXTRA\n        super(PoseHighResolutionNet, self).__init__()\n\n        # stem net\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(Bottleneck, 64, 4)\n\n        self.stage2_cfg = cfg['MODEL']['EXTRA']['STAGE2']\n        num_channels = self.stage2_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage2_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition1 = self._make_transition_layer([256], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg['MODEL']['EXTRA']['STAGE3']\n        num_channels = self.stage3_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage3_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg['MODEL']['EXTRA']['STAGE4']\n        num_channels = self.stage4_cfg['NUM_CHANNELS']\n        block = blocks_dict[self.stage4_cfg['BLOCK']]\n        num_channels = [\n            num_channels[i] * block.expansion for i in range(len(num_channels))\n        ]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels, multi_scale_output=False)\n\n        self.final_layer = nn.Conv2d(\n            in_channels=pre_stage_channels[0],\n            out_channels=cfg.MODEL.NUM_JOINTS,\n            kernel_size=extra.FINAL_CONV_KERNEL,\n            stride=1,\n            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0\n        )\n\n        self.pretrained_layers = cfg['MODEL']['EXTRA']['PRETRAINED_LAYERS']\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                3, 1, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i+1-num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i-num_branches_pre else inchannels\n                    conv3x3s.append(\n                        nn.Sequential(\n                            nn.Conv2d(\n                                inchannels, outchannels, 3, 2, 1, bias=False\n                            ),\n                            nn.BatchNorm2d(outchannels),\n                            nn.ReLU(inplace=True)\n                        )\n                    )\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.inplanes, planes * block.expansion,\n                    kernel_size=1, stride=stride, bias=False\n                ),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels,\n                    multi_scale_output=True):\n        num_modules = layer_config['NUM_MODULES']\n        num_branches = layer_config['NUM_BRANCHES']\n        num_blocks = layer_config['NUM_BLOCKS']\n        num_channels = layer_config['NUM_CHANNELS']\n        block = blocks_dict[layer_config['BLOCK']]\n        fuse_method = layer_config['FUSE_METHOD']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(\n                HighResolutionModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    num_inchannels,\n                    num_channels,\n                    fuse_method,\n                    reset_multi_scale_output\n                )\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg['NUM_BRANCHES']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg['NUM_BRANCHES']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg['NUM_BRANCHES']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        x = self.final_layer(y_list[0])\n\n        return x\n\n    def init_weights(self, pretrained=''):\n        logger.info('=> init weights from normal distribution')\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n                for name, _ in m.named_parameters():\n                    if name in ['bias']:\n                        nn.init.constant_(m.bias, 0)\n\n        if os.path.isfile(pretrained):\n            pretrained_state_dict = torch.load(pretrained)\n            logger.info('=> loading pretrained model {}'.format(pretrained))\n\n            need_init_state_dict = {}\n            for name, m in pretrained_state_dict.items():\n                if name.split('.')[0] in self.pretrained_layers \\\n                   or self.pretrained_layers[0] is '*':\n                    need_init_state_dict[name] = m\n            self.load_state_dict(need_init_state_dict, strict=False)\n        elif pretrained:\n            logger.error('=> please download pre-trained models first!')\n            raise ValueError('{} is not exist!'.format(pretrained))\n\n\ndef get_pose_net(cfg, is_train, **kwargs):\n    model = PoseHighResolutionNet(cfg, **kwargs)\n\n    if is_train and cfg.MODEL.INIT_WEIGHTS:\n        model.init_weights(cfg.MODEL.PRETRAINED)\n\n    return model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-05-18T13:06:06.360361Z","iopub.execute_input":"2024-05-18T13:06:06.360638Z","iopub.status.idle":"2024-05-18T13:06:06.435334Z","shell.execute_reply.started":"2024-05-18T13:06:06.360614Z","shell.execute_reply":"2024-05-18T13:06:06.434387Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"<>:470: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n<>:470: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n/tmp/ipykernel_33/1592351585.py:470: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n  or self.pretrained_layers[0] is '*':\n","output_type":"stream"}]},{"cell_type":"code","source":"def flip_back(output_flipped, matched_parts):\n    '''\n    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)\n    '''\n    assert output_flipped.ndim == 4,\\\n        'output_flipped should be [batch_size, num_joints, height, width]'\n\n    output_flipped = output_flipped[:, :, :, ::-1]\n\n    for pair in matched_parts:\n        tmp = output_flipped[:, pair[0], :, :].copy()\n        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\n        output_flipped[:, pair[1], :, :] = tmp\n\n    return output_flipped\n\n\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\n    \"\"\"\n    flip coords\n    \"\"\"\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = \\\n            joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints*joints_vis, joints_vis\n\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef get_affine_transform(\n        center, scale, rot, output_size,\n        shift=np.array([0, 0], dtype=np.float32), inv=0\n):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n\n    src_dir = get_dir([0, (src_w-1) * -0.5], rot_rad)\n    dst_dir = np.array([0, (dst_w-1) * -0.5], np.float32)\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [(dst_w-1) * 0.5, (dst_h-1) * 0.5]\n    dst[1, :] = np.array([(dst_w-1) * 0.5, (dst_h-1) * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef crop(img, center, scale, output_size, rot=0):\n    trans = get_affine_transform(center, scale, rot, output_size)\n\n    dst_img = cv2.warpAffine(\n        img, trans, (int(output_size[0]), int(output_size[1])),\n        flags=cv2.INTER_LINEAR\n    )\n\n    return dst_img\ndef get_max_preds(batch_heatmaps):\n    '''\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    '''\n    assert isinstance(batch_heatmaps, np.ndarray), \\\n        'batch_heatmaps should be numpy.ndarray'\n    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef taylor(hm, coord):\n    heatmap_height = hm.shape[0]\n    heatmap_width = hm.shape[1]\n    px = int(coord[0])\n    py = int(coord[1])\n    if 1 < px < heatmap_width-2 and 1 < py < heatmap_height-2:\n        dx  = 0.5 * (hm[py][px+1] - hm[py][px-1])\n        dy  = 0.5 * (hm[py+1][px] - hm[py-1][px])\n        dxx = 0.25 * (hm[py][px+2] - 2 * hm[py][px] + hm[py][px-2])\n        dxy = 0.25 * (hm[py+1][px+1] - hm[py-1][px+1] - hm[py+1][px-1] \\\n            + hm[py-1][px-1])\n        dyy = 0.25 * (hm[py+2*1][px] - 2 * hm[py][px] + hm[py-2*1][px])\n        derivative = np.matrix([[dx],[dy]])\n        hessian = np.matrix([[dxx,dxy],[dxy,dyy]])\n        if dxx * dyy - dxy ** 2 != 0:\n            hessianinv = hessian.I\n            offset = -hessianinv * derivative\n            offset = np.squeeze(np.array(offset.T), axis=0)\n            coord += offset\n    return coord\n\n\ndef gaussian_blur(hm, kernel):\n    border = (kernel - 1) // 2\n    batch_size = hm.shape[0]\n    num_joints = hm.shape[1]\n    height = hm.shape[2]\n    width = hm.shape[3]\n    for i in range(batch_size):\n        for j in range(num_joints):\n            origin_max = np.max(hm[i,j])\n            dr = np.zeros((height + 2 * border, width + 2 * border))\n            dr[border: -border, border: -border] = hm[i,j].copy()\n            dr = cv2.GaussianBlur(dr, (kernel, kernel), 0)\n            hm[i,j] = dr[border: -border, border: -border].copy()\n            hm[i,j] *= origin_max / np.max(hm[i,j])\n    return hm\n\n\ndef get_final_preds(config, hm, center, scale):\n    coords, maxvals = get_max_preds(hm)\n    heatmap_height = hm.shape[2]\n    heatmap_width = hm.shape[3]\n\n    # post-processing\n    hm = gaussian_blur(hm, config.TEST.BLUR_KERNEL)\n    hm = np.maximum(hm, 1e-10)\n    hm = np.log(hm)\n    for n in range(coords.shape[0]):\n        for p in range(coords.shape[1]):\n            coords[n,p] = taylor(hm[n][p], coords[n][p])\n\n    preds = coords.copy()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = transform_preds(\n            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\n        )\n\n    return preds, maxvals","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:06:06.438074Z","iopub.execute_input":"2024-05-18T13:06:06.438364Z","iopub.status.idle":"2024-05-18T13:06:06.479444Z","shell.execute_reply.started":"2024-05-18T13:06:06.438340Z","shell.execute_reply":"2024-05-18T13:06:06.478478Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# function definition\ndef cosine_similarity(a, b):\n  cos = ( np.dot(a, b) / \n          (np.linalg.norm(a) * np.linalg.norm(b)) )\n  if cos < -1:\n    return -1\n  elif cos > 1:\n    return 1\n  else:\n    return cos\n\n\ndef angle_degree(a, b):\n  angle = (np.arccos(cosine_similarity(a, b)) / \n          np.pi) * 180\n  if angle > 180:\n    return 180.\n  if angle < 0:\n    return 0\n  else:\n    return angle\n\n# @parameter\n# part: contain 2 joint coordinate [[x0, y0], [x1, y1]]\ndef return_vector(part_id, skeleton):\n  point_a = skeleton[part_id[0]]\n  point_b = skeleton[part_id[1]]\n  return point_a - point_b\n\ndef is_in_range(num, range1, range2):\n  ceil = max(range1, range2)\n  floor = min(range1, range2)\n  return (num >= floor and num <= ceil)\n\ndef find_pose_id(ske_pred, keyparts, stages):\n  assert ske_pred.shape == (NUM_KPTS, 2)\n  avg_angle = 0\n  for keypart in keyparts:\n    part_a, part_b = keypart\n    vec_a = return_vector(SKELETON[part_a], ske_pred)\n    vec_b = return_vector(SKELETON[part_b], ske_pred)\n\n    if sum(vec_a == 0) == 2 or sum(vec_b == 0) == 2:\n      return -1\n    \n    pred_angle = angle_degree(vec_a, vec_b)\n    avg_angle += pred_angle\n\n  avg_angle /= len(keyparts)\n  for i, stage in enumerate(stages):\n    if is_in_range(avg_angle, stage[0], stage[1]):\n      return i\n\ndef compare_skeleton(ske_true, ske_pred, excer_part, threshold= 10):\n  assert ske_true.shape == (NUM_KPTS, 2)\n  assert ske_pred.shape == (NUM_KPTS, 2)\n  part_flag = []\n  for part in excer_part:\n    kpt_a, kpt_b = SKELETON[part]\n    true_vec = ske_true[kpt_a] - ske_true[kpt_b]\n    pred_vec = ske_pred[kpt_a] - ske_pred[kpt_b]\n    angle = angle_degree(true_vec, pred_vec)\n    part_flag.append(True if angle <= threshold else False)\n  return part_flag\n\ndef draw_pose(keypoints, part_flag, excer_part, img, joint_thickness=6):\n  assert keypoints.shape == (NUM_KPTS, 2)\n  for i, part in enumerate(excer_part):\n    kpt_a, kpt_b = SKELETON[part]\n    c = COLOR[\"blue\"] if part_flag[i] else COLOR[\"red\"]\n    x_a, y_a = keypoints[kpt_a]\n    x_b, y_b = keypoints[kpt_b]\n    cv2.circle(img, (int(x_a), int(y_a)), joint_thickness, COLOR[\"green\"], -1)\n    cv2.circle(img, (int(x_b), int(y_b)), joint_thickness, COLOR[\"green\"], -1)\n    cv2.line(img, (int(x_a), int(y_a)), (int(x_b), int(y_b)), c, 2)\n\ndef get_pose_estimation_prediction(pose_model, image, center, scale):\n  scale = np.array([scale, scale])\n  rotation = 0\n\n  # pose estimation transformation\n  trans = get_affine_transform(center, scale, rotation, cfg.MODEL.IMAGE_SIZE)\n  model_input = cv2.warpAffine(\n    image,\n    trans,\n    (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n    flags=cv2.INTER_LINEAR)\n  transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                          std=[0.229, 0.224, 0.225]),\n  ])\n\n  # pose estimation inference\n  model_input = transform(model_input).unsqueeze(0)\n  # switch to evaluate mode\n  pose_model.eval()\n  with torch.no_grad():\n    # compute output heatmap\n    output = pose_model(model_input)\n    preds, _ = get_final_preds(\n      cfg,\n      output.clone().cpu().numpy(),\n      np.asarray([center]),\n      np.asarray([scale]))\n\n    return preds\n\ndef calculate_center_scale(box):\n  x1, y1, x2, y2 = box\n  center_x = (x2 + x1) / 2\n  center_y = (y2 + y1) / 2\n  width = x2 - x1\n  height = y2 - y1\n  \n  center = np.array([center_x, center_y], dtype=np.float32)\n  scale = max(width, height) / 200\n\n  return center, scale\n\ndef get_person_box(model, img):\n  results = model(img)\n  \n  data = results.xyxy[0].cpu().numpy()\n    \n  if len(data) == 0:\n    return np.array([-1])\n\n  max_index = np.argmax(data[:, 4])\n  max_element = data[max_index]\n\n  center, scale = calculate_center_scale(max_element[:4])\n\n  return center, scale\n    \ndef load_yolo(version):\n  yolo = torch.hub.load('ultralytics/yolov5', version, \n                        pretrained= True, _verbose= False)\n  yolo.cuda()\n  yolo.classes = [0]\n  return yolo\n\ndef load_hrnet(cfg):\n  pose_model = get_pose_net(cfg, is_train=False)\n    \n  print(cfg.TEST.MODEL_FILE)\n  if cfg.TEST.MODEL_FILE:\n    print('=> loading model from {}'.format(cfg.TEST.MODEL_FILE))\n    pose_model.load_state_dict(torch.load(cfg.TEST.MODEL_FILE), strict=False)\n  else:\n    print('expected model defined in config at TEST.MODEL_FILE')\n\n  pose_model = torch.nn.DataParallel(pose_model, device_ids=cfg.GPUS)\n  pose_model.to(CTX)\n  pose_model.eval()\n\n  return pose_model\n\ndef load_true_poses(path):\n  joints_path = os.path.join(path, \"joints.csv\")\n  \n  img_path = os.path.join(path, \"images\")\n  \n  img_paths = os.listdir(img_path)\n  img_paths.sort(key= lambda x: int(x[0]))\n  imgs = [cv2.imread(os.path.join(img_path, p)) for p in img_paths]\n  skes = pd.read_csv(joints_path).to_numpy().reshape(len(imgs), NUM_KPTS, 2)\n  return imgs, skes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T13:06:06.481055Z","iopub.execute_input":"2024-05-18T13:06:06.481329Z","iopub.status.idle":"2024-05-18T13:06:06.511866Z","shell.execute_reply.started":"2024-05-18T13:06:06.481305Z","shell.execute_reply":"2024-05-18T13:06:06.510909Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Demo","metadata":{}},{"cell_type":"code","source":"def video_test(video_path, output_path, pose_path, angle_threshold=5):\n  box_model = load_yolo(\"yolov5s\")\n  pose_model = load_hrnet(cfg)\n  true_pose_images, true_pose_skes = load_true_poses(pose_path)\n\n  video_cap = cv2.VideoCapture(video_path)\n  if not video_cap.isOpened():\n    print(\"Cannot open this video.\")\n    return\n\n  frame_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n  frame_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n  fps = 24.0\n  video_out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), \n                        fps, (frame_width * 2, frame_height))\n  count = 0\n  while True:\n    pose_id = 0\n      \n    ret, image_bgr = video_cap.read()\n    if not ret:\n      break\n    \n    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    center, scale = get_person_box(box_model, image)\n\n    if center[0] == -1:\n     continue\n\n    image_pose = image.copy() if cfg.DATASET.COLOR_RGB else image_bgr.copy()\n    pose_pred = get_pose_estimation_prediction(pose_model, image_pose, center, scale)[0]\n    pose_id = find_pose_id(pose_pred, SQUAT_KEYPART, STAGE_ANGLE)\n    if pose_id == -1:\n      continue\n    \n    true_pose_image = true_pose_images[pose_id]\n    true_skeleton = true_pose_skes[pose_id]\n    part_flag = compare_skeleton(true_skeleton, pose_pred, SQUAT_PART, threshold= angle_threshold)\n    draw_pose(pose_pred, part_flag, SQUAT_PART, image_bgr)\n    \n    true_pose_resized = cv2.resize(true_pose_image, (frame_width, frame_height))\n    combined_frame = np.hstack((image_bgr, true_pose_resized))\n    video_out.write(combined_frame)\n    \n    count += 1\n    if count % 24 == 0:\n      print(count)\n  \n  video_cap.release()\n  video_out.release()\n  print(\"Video processing complete. Output saved to\", output_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:06:06.512978Z","iopub.execute_input":"2024-05-18T13:06:06.513280Z","iopub.status.idle":"2024-05-18T13:06:06.528723Z","shell.execute_reply.started":"2024-05-18T13:06:06.513254Z","shell.execute_reply":"2024-05-18T13:06:06.527730Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"input_video = \"/kaggle/input/hrpose-input/tue.mp4\"\noutput_video = \"output_video.mp4\"\nsquat_dir = \"/kaggle/input/hrpose-input/squat\"\nvideo_test(input_video, output_video, squat_dir)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T13:06:06.529961Z","iopub.execute_input":"2024-05-18T13:06:06.530320Z","iopub.status.idle":"2024-05-18T13:07:14.150612Z","shell.execute_reply.started":"2024-05-18T13:06:06.530288Z","shell.execute_reply":"2024-05-18T13:07:14.149168Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/darkpose/w32_256Ã—256.pth\n=> loading model from /kaggle/input/darkpose/w32_256Ã—256.pth\n24\n48\n72\n96\n120\n144\n168\n192\n216\n240\n264\n288\n312\n336\n360\n384\n408\n432\n456\n480\n504\n528\n552\n576\n600\n624\n648\n672\n696\n720\nVideo processing complete. Output saved to output_video.mp4\n","output_type":"stream"}]}]}